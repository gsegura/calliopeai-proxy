# Server Configuration
PORT=3000

# API Keys for proxied services
# SearchAPI.io API Key (for /api/web endpoint)
SEARCHAPI_API_KEY=your_searchapi_api_key_here

#-------------------------------------------------------------------------------
# LLM Provider API Keys for Model Proxy (/model-proxy/v1)
#-------------------------------------------------------------------------------
# The Calliope Proxy uses the `apiKeyLocation` property in the request body
# (e.g., "env:OPENAI_API_KEY") to determine which environment variable to use.
# Ensure the variable names here match what you expect to pass in `apiKeyLocation`.

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Vertex AI (Note: Vertex AI often uses service account keys or gcloud auth)
# If using a long-lived API key, provide it here.
# For service accounts, the application might need to be configured
# to use GOOGLE_APPLICATION_CREDENTIALS environment variable pointing to a JSON key file.
# The proxy logic currently expects a simple Bearer token style API key.
#VERTEX_AI_API_KEY=your_vertex_ai_api_key_here
#GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-file.json

# Ollama (Ollama typically runs locally and may not require an API key by default)
# If your Ollama instance is secured with an API key, provide it here.
#OLLAMA_API_KEY=your_ollama_api_key_if_any

# Azure OpenAI Service
# Azure OpenAI uses a specific endpoint and an API key.
# The `apiBase` in calliopeProperties should point to your Azure OpenAI resource endpoint
# (e.g., https://your-resource-name.openai.azure.com)
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here


# Add other environment variables as needed by your application
# For example, database connection strings, logging levels, etc.
# LOG_LEVEL=info
